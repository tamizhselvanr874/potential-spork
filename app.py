import streamlit as st  
import requests  
from PIL import Image  
from io import BytesIO  
import base64  
import logging  
import time  
  
# Configure logging  
logging.basicConfig(level=logging.INFO)  
  
# Set OpenAI API configuration
openai.api_key = AZURE_OPENAI_API_KEY
openai.api_type = OPENAI_API_TYPE
openai.api_base = AZURE_OPENAI_ENDPOINT
openai.api_version = OPENAI_API_VERSION
IMAGE_GENERATION_URL = os.getenv("IMAGE_GENERATION_URL", "default_url")
  
# AzureOpenAI Client Setup  
class AzureOpenAI:  
    def __init__(self, azure_endpoint, api_key, api_version):  
        self.azure_endpoint = azure_endpoint  
        self.api_key = api_key  
        self.api_version = api_version  
  
    def chat_completion(self, model, messages, temperature, max_tokens):  
        url = f"{self.azure_endpoint}/openai/deployments/{model}/chat/completions?api-version={self.api_version}"  
        headers = {  
            "Content-Type": "application/json",  
            "api-key": self.api_key,  
        }  
        data = {  
            "messages": messages,  
            "temperature": temperature,  
            "max_tokens": max_tokens,  
        }  
        base_delay = 1  
        max_delay = 32  
        max_attempts = 5  
  
        for attempt in range(max_attempts):  
            try:  
                response = requests.post(url, headers=headers, json=data)  
                response.raise_for_status()  
                return response.json()  
            except requests.exceptions.RequestException as e:  
                logging.error(f"Attempt {attempt + 1} failed: {str(e)}")  
                if attempt < max_attempts - 1:  
                    delay = min(base_delay * 2 ** attempt, max_delay)  
                    logging.info(f"Retrying in {delay} seconds...")  
                    time.sleep(delay)  
                else:  
                    raise  
  
client = AzureOpenAI(  
    azure_endpoint=azure_endpoint,  
    api_key=api_key,  
    api_version=api_version,  
)  
  
IMAGE_GENERATION_URL = "https://afsimage.azurewebsites.net/api/httpTriggerts"  
  
# Initialize session state variables  
if 'messages' not in st.session_state:  
    st.session_state.messages = []  
if 'current_question_index' not in st.session_state:  
    st.session_state.current_question_index = 0  
if 'final_prompt' not in st.session_state:  
    st.session_state.final_prompt = None  
  
# Define prompt categories and options  
PROMPT_CATEGORIES = {  
    "Nature and Landscapes": [  
        ("Forests", "A mystical forest during twilight, dense fog weaving through towering ancient trees, glowing mushrooms scattered across the forest floor, ethereal light beams breaking through the canopy."),  
        ("Mountains", "A breathtaking snow-capped mountain range at sunrise, with golden light illuminating the peaks and a serene blue lake reflecting the view below."),  
        ("Beaches", "A tranquil tropical beach at sunset, with vibrant orange and pink hues painting the sky, crystal-clear turquoise water, and a wooden pier extending into the ocean.")  
    ],  
    "Architecture": [  
        ("Futuristic Cities", "A sprawling cyberpunk city at night, with neon-lit skyscrapers, flying cars, bustling streets filled with holographic signs, and a vibrant nightlife."),  
        ("Historical Monuments", "A beautifully detailed Roman colosseum at dusk, surrounded by lush greenery and tourists admiring the historic grandeur."),  
        ("Fantasy Castles", "An enormous floating castle in the sky, surrounded by fluffy white clouds, glowing waterfalls cascading from its edges, and magical birds flying around.")  
    ],  
    # Add more categories as needed  
}  
  
st.title("Interactive Image Chat Generation")  
  
def encode_image(image):  
    buffered = BytesIO()  
    image.save(buffered, format="PNG")  
    return base64.b64encode(buffered.getvalue()).decode("utf-8")  
  
def get_image_explanation(base64_image):  
    headers = {  
        "Content-Type": "application/json",  
        "api-key": api_key  
    }  
    data = {  
        "model": model,  
        "messages": [  
            {"role": "system", "content": "You are a helpful assistant that describes images."},  
            {"role": "user", "content": [  
                {"type": "text", "text": "Explain the content of this image in a single, coherent paragraph. The explanation should be concise and semantically meaningful, summarizing all major points from the image in one paragraph. Avoid using bullet points or separate lists."},  
                {"type": "image_url", "image_url": {"url": f"data:image/png;base64,{base64_image}"}}  
            ]}  
        ],  
        "temperature": 0.7  
    }  
  
    try:  
        response = requests.post(  
            f"{azure_endpoint}/openai/deployments/{model}/chat/completions?api-version={api_version}",  
            headers=headers,  
            json=data  
        )  
        response.raise_for_status()  
        result = response.json()  
        explanation = result["choices"][0]["message"]["content"]  
        return explanation  
    except requests.exceptions.HTTPError as http_err:  
        logging.error(f"HTTP error occurred: {http_err}")  
    except Exception as e:  
        logging.error(f"An error occurred: {e}")  
    return "Failed to get image explanation."  
  
def call_azure_openai(messages, max_tokens, temperature):  
    try:  
        response = client.chat_completion(  
            model=model,  
            messages=messages,  
            temperature=temperature,  
            max_tokens=max_tokens  
        )  
        return response['choices'][0]['message']['content'].strip()  
    except Exception as e:  
        return f"Error: {str(e)}"  
  
def finalize_prompt(conversation):  
    prompt = "Craft a concise and comprehensive image prompt using the specific details provided by the user in the conversation. "  
    prompt += "Incorporate all relevant graphical elements discussed, such as colors, textures, shapes, lighting, depth, and style, without making assumptions or adding speculative details. "  
    prompt += "Ensure the prompt is clear, structured, and accurately reflects the user's inputs. "  
    prompt += "End by asking: 'Are you okay with the prompt? Are there any things that need to be adjusted?'"  
    for turn in conversation:  
        if turn['role'] == 'user':  
            prompt += f"User: {turn['content']}. "  
        elif turn['role'] == 'assistant':  
            prompt += f"Assistant: {turn['content']}. "  
    messages = [  
        {"role": "system", "content": "You are a helpful AI assistant."},  
        {"role": "user", "content": prompt}  
    ]  
    return call_azure_openai(messages, 750, 0.7)  
  
def generate_image(prompt):  
    try:  
        response = requests.post(  
            IMAGE_GENERATION_URL,  
            json={"prompt": prompt},  
            headers={"Content-Type": "application/json"}  
        )  
        if response.status_code == 200:  
            data = response.json()  
            if "imageUrls" in data and data['imageUrls']:  
                return data['imageUrls'][0]  
        return "Failed to generate image."  
    except Exception as e:  
        return f"Error: {str(e)}"  
  
def display_image_options(image_url, image_caption):  
    if image_url:  
        st.sidebar.image(image_url, caption=image_caption, use_column_width=True)  
        image_data = requests.get(image_url).content  
        st.sidebar.download_button(  
            label=f"Download {image_caption}",  
            data=image_data,  
            file_name=f"{image_caption.lower().replace(' ', '_')}.png",  
            mime="image/png"  
        )  
  
def handle_image_input(image_file):  
    if image_file:  
        image = Image.open(image_file)  
        encoded_image = encode_image(image)  
        explanation = get_image_explanation(encoded_image)  
        st.session_state.messages.append({"role": "assistant", "content": explanation})  
  
def display_prompt_library():  
    with st.sidebar:  
        st.write("*Prompt Library:*")  
        for category, prompts in PROMPT_CATEGORIES.items():  
            st.write(f"### {category}")  
            for title, prompt in prompts:  
                if st.button(title):  
                    st.session_state.selected_prompt = prompt  
                    st.session_state.messages.append({"role": "assistant", "content": f"Let's start refining your idea: {prompt}"})  
                    st.session_state.awaiting_followup_response = True  
                    return  
  
def chat_interface():  
    image_file = st.sidebar.file_uploader("Upload an image for explanation and refinement:", type=["png", "jpg", "jpeg"])  
    user_input = st.chat_input("Your message:")  
  
    if image_file:  
        handle_image_input(image_file)  
        if user_input:  
            st.session_state.messages.append({"role": "user", "content": user_input})  
            # Directly finalize prompt after image explanation and user input  
            st.session_state.final_prompt = finalize_prompt(st.session_state.messages)  
            st.session_state.messages.append({"role": "assistant", "content": f"*Final Prompt:* {st.session_state.final_prompt}"})  
  
    elif user_input:  
        st.session_state.messages.append({"role": "user", "content": user_input})  
  
        # Existing logic with dynamic questions  
        if st.session_state.current_question_index < 6:  # Ask 6 questions  
            context = ' '.join([msg['content'] for msg in st.session_state.messages])  
            dynamic_question = generate_dynamic_questions(user_input, context)  
            st.session_state.messages.append({"role": "assistant", "content": dynamic_question})  
            st.session_state.current_question_index += 1  
        else:  
            st.session_state.final_prompt = finalize_prompt(st.session_state.messages)  
            st.session_state.messages.append({"role": "assistant", "content": f"*Final Prompt:* {st.session_state.final_prompt}"})  
  
    display_prompt_library()  
  
    for message in st.session_state.messages:  
        with st.chat_message(message["role"]):  
            st.markdown(message["content"])  
  
    if st.session_state.final_prompt and st.button("Generate Image"):  
        image_url = generate_image(st.session_state.final_prompt)  
        if image_url and "Error" not in image_url:  
            st.session_state.generated_image_url = image_url  
            display_image_options(image_url, "Generated Image")  
        else:  
            st.write("Failed to generate image.")  
  
def generate_dynamic_questions(user_input, conversation_history):  
    prompt = f"""  
    We are working with the initial concept: "{user_input}".  
    Given the conversation so far: "{conversation_history}", generate a follow-up question or suggestion that explores one of the following aspects: colors, textures, shapes, lighting, depth, or style.  
    The question should be engaging, concise and encourage the user to think creatively about their concept.  
    Additionally, provide a short recommendation to inspire the user further.  
    """  
    messages = [  
        {"role": "system", "content": "You are a creative assistant who generates insightful questions to refine image prompts, along with concise recommendations."},  
        {"role": "user", "content": prompt}  
    ]  
    response_content = call_azure_openai(messages, 750, 0.8)  
    question, recommendation = response_content.split("Recommendation:", 1)  
    return f"{question.strip()}Recommendation:{recommendation.strip()}"  
  
chat_interface()  
